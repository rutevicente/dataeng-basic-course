{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucprosa/dataeng-basic-course/blob/main/spark_streaming/challenges/final_challenges.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "a1a32492-7ada-41e3-b55b-625139569bd5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udk3tohSaXOH",
        "outputId": "ee783d99-ffac-49ad-e9e5-4cd49c0dd7b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-33.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from faker) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.16.0)\n",
            "Downloading Faker-33.1.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-33.1.0\n"
          ]
        }
      ],
      "source": [
        "%pip install faker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcybt71kTDNt"
      },
      "source": [
        "# Context\n",
        "Message events are coming from platform message broker (kafka, pubsub, kinesis...).\n",
        "You need to process the data according to the requirements.\n",
        "\n",
        "Message schema:\n",
        "- timestamp\n",
        "- value\n",
        "- event_type\n",
        "- message_id\n",
        "- country_id\n",
        "- user_id\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkyPORKNSYvV"
      },
      "source": [
        "# Challenge 1\n",
        "\n",
        "Step 1\n",
        "- Change exising producer\n",
        "\t- Change parquet location to \"/content/lake/bronze/messages/data\"\n",
        "\t- Add checkpoint (/content/lake/bronze/messages/checkpoint)\n",
        "\t- Delete /content/lake/bronze/messages and reprocess data\n",
        "\t- For reprocessing, run the streaming for at least 1 minute, then stop it\n",
        "\n",
        "Step 2\n",
        "- Implement new stream job to read from messages in bronze layer and split result in two locations\n",
        "\t- \"messages_corrupted\"\n",
        "\t\t- logic: event_status is null, empty or equal to \"NONE\"\n",
        "\t\t- extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages_corrupted/data\n",
        "\n",
        "\t- \"messages\"\n",
        "\t\t- logic: not corrupted data\n",
        "\t\t- extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages/data\n",
        "\n",
        "\t- technical requirements\n",
        "\t\t- add checkpoint (choose location)\n",
        "\t\t- use StructSchema\n",
        "\t\t- Set trigger interval to 5 seconds\n",
        "\t\t- run streaming for at least 20 seconds, then stop it\n",
        "\n",
        "\t- alternatives\n",
        "\t\t- implementing single streaming job with foreach/- foreachBatch logic to write into two locations\n",
        "\t\t- implementing two streaming jobs, one for messages and another for messages_corrupted\n",
        "\t\t- (paying attention on the paths and checkpoints)\n",
        "\n",
        "\n",
        "  - Check results:\n",
        "    - results from messages in bronze layer should match with the sum of messages+messages_corrupted in the silver layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDGMKwBdi1qy"
      },
      "source": [
        "## Producer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tPCOdivrfhYh"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "from faker import Faker\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName('Test streaming').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "# Create instance Faker\n",
        "fake = Faker()\n",
        "messages = [fake.uuid4() for _ in range(50)]\n",
        "\n",
        "def enrich_data(df, messages=messages):\n",
        "    fake = Faker()\n",
        "    new_columns = {\n",
        "        'event_type': F.lit(fake.random_element(elements=('OPEN', 'RECEIVED', 'SENT', 'CREATED', 'CLICKED', '', 'NONE'))),\n",
        "        'message_id': F.lit(fake.random_element(elements=messages)),\n",
        "        'channel': F.lit(fake.random_element(elements=('CHAT', 'EMAIL', 'SMS', 'PUSH', 'OTHER'))),\n",
        "        'country_id': F.lit(fake.random_int(min=2000, max=2015)),\n",
        "        'user_id': F.lit(fake.random_int(min=1000, max=1050)),\n",
        "    }\n",
        "\n",
        "\n",
        "    for col_name, col_value in new_columns.items():\n",
        "        df = df.withColumn(col_name, col_value)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def insert_messages(df: DataFrame, batch_id):\n",
        "    enrich = enrich_data(df)\n",
        "    enrich.write.mode(\"append\").format(\"parquet\").save(\"/content/lake/bronze/messages/data\")\n",
        "\n",
        "\n",
        "def clean_directories_with_shutil():\n",
        "    parquet_location = \"/content/lake/bronze/messages/data\"\n",
        "    checkpoint_location = \"/content/lake/bronze/messages/checkpoint\"\n",
        "\n",
        "    # Check and remove directories\n",
        "    if os.path.exists(parquet_location):\n",
        "        shutil.rmtree(parquet_location)  # Remove any old Parquet directory\n",
        "    if os.path.exists(checkpoint_location):\n",
        "        shutil.rmtree(checkpoint_location)  # Remove any old Checkpoint directoty\n",
        "\n",
        "# Remove any old data\n",
        "clean_directories_with_shutil()\n",
        "\n",
        "# readStream the data\n",
        "df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
        "\n",
        "# writeStream the data and create the Checkpoint\n",
        "query = (df_stream.writeStream\n",
        "    .outputMode('append')\n",
        "    .trigger(processingTime='1 seconds')\n",
        "    .foreachBatch(insert_messages)\n",
        "    .option(\"checkpointLocation\", \"/content/lake/bronze/messages/checkpoint\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "# Run streaming for 1 minute (60 seconds)\n",
        "query.awaitTermination(60)\n",
        "\n",
        "# Stop streaming\n",
        "query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZWQExsnzlMFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41d3e04c-38c0-42d0-b9ac-59df99c00651"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|           timestamp|value|event_type|          message_id|channel|country_id|user_id|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|2024-12-11 17:45:...|    1|      OPEN|34b86fce-5f61-4c3...|   CHAT|      2011|   1022|\n",
            "|2024-12-11 17:45:...|    3|      OPEN|34b86fce-5f61-4c3...|   CHAT|      2011|   1022|\n",
            "|2024-12-11 17:45:...|    5|      OPEN|34b86fce-5f61-4c3...|   CHAT|      2011|   1022|\n",
            "|2024-12-11 17:45:...|    0|      OPEN|34b86fce-5f61-4c3...|   CHAT|      2011|   1022|\n",
            "|2024-12-11 17:45:...|    2|      OPEN|34b86fce-5f61-4c3...|   CHAT|      2011|   1022|\n",
            "|2024-12-11 17:45:...|    4|      OPEN|34b86fce-5f61-4c3...|   CHAT|      2011|   1022|\n",
            "|2024-12-11 17:45:...|   25|  RECEIVED|8239fc3e-91ba-4cb...|  EMAIL|      2014|   1010|\n",
            "|2024-12-11 17:45:...|   29|  RECEIVED|f4b7d7d6-86ad-429...|  OTHER|      2000|   1009|\n",
            "|2024-12-11 17:45:...|   32|  RECEIVED|b8fb4080-954d-4b6...|  OTHER|      2002|   1036|\n",
            "|2024-12-11 17:45:...|   35|  RECEIVED|8239fc3e-91ba-4cb...|   PUSH|      2003|   1015|\n",
            "|2024-12-11 17:45:...|   46|   CLICKED|764a9b82-e5de-4b6...|  EMAIL|      2001|   1033|\n",
            "|2024-12-11 17:45:...|   14|  RECEIVED|a2972c89-cb37-44b...|   PUSH|      2012|   1034|\n",
            "|2024-12-11 17:45:...|   12|  RECEIVED|13f32c32-48bc-40b...|   CHAT|      2008|   1007|\n",
            "|2024-12-11 17:45:...|   33|   CLICKED|1366c1e2-c7f5-408...|  OTHER|      2008|   1038|\n",
            "|2024-12-11 17:45:...|   50|  RECEIVED|ea57c6b0-7a25-49f...|   PUSH|      2002|   1013|\n",
            "|2024-12-11 17:45:...|   34|   CLICKED|2a8c1157-af0b-4b7...|  OTHER|      2014|   1000|\n",
            "|2024-12-11 17:45:...|   42|   CLICKED|e5b5ef2c-e0ce-44e...|  OTHER|      2004|   1027|\n",
            "|2024-12-11 17:45:...|   15|   CLICKED|3190948e-072b-408...|   PUSH|      2010|   1013|\n",
            "|2024-12-11 17:45:...|   13|   CREATED|aff1d91b-7975-42e...|   PUSH|      2010|   1009|\n",
            "|2024-12-11 17:45:...|   17|   CREATED|aff1d91b-7975-42e...|   PUSH|      2003|   1047|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_bronze = spark.read.format(\"parquet\").load(\"/content/lake/bronze/messages/data\")\n",
        "df_bronze.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RraxHCycMdEZ"
      },
      "source": [
        "## Additional datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cfsus3dxMcQI"
      },
      "outputs": [],
      "source": [
        "countries = [\n",
        "    {\"country_id\": 2000, \"country\": \"Brazil\"},\n",
        "    {\"country_id\": 2001, \"country\": \"Portugal\"},\n",
        "    {\"country_id\": 2002, \"country\": \"Spain\"},\n",
        "    {\"country_id\": 2003, \"country\": \"Germany\"},\n",
        "    {\"country_id\": 2004, \"country\": \"France\"},\n",
        "    {\"country_id\": 2005, \"country\": \"Italy\"},\n",
        "    {\"country_id\": 2006, \"country\": \"United Kingdom\"},\n",
        "    {\"country_id\": 2007, \"country\": \"United States\"},\n",
        "    {\"country_id\": 2008, \"country\": \"Canada\"},\n",
        "    {\"country_id\": 2009, \"country\": \"Australia\"},\n",
        "    {\"country_id\": 2010, \"country\": \"Japan\"},\n",
        "    {\"country_id\": 2011, \"country\": \"China\"},\n",
        "    {\"country_id\": 2012, \"country\": \"India\"},\n",
        "    {\"country_id\": 2013, \"country\": \"South Korea\"},\n",
        "    {\"country_id\": 2014, \"country\": \"Russia\"},\n",
        "    {\"country_id\": 2015, \"country\": \"Argentina\"}\n",
        "]\n",
        "\n",
        "countries = spark.createDataFrame(countries)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the schema of df_bronze\n",
        "df_bronze.schema"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zW3H6mKKU83L",
        "outputId": "31b084d2-662a-4ef5-b499-f6a7ffa70557"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('timestamp', TimestampType(), True), StructField('value', LongType(), True), StructField('event_type', StringType(), True), StructField('message_id', StringType(), True), StructField('channel', StringType(), True), StructField('country_id', IntegerType(), True), StructField('user_id', IntegerType(), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, LongType, StringType, TimestampType, IntegerType\n",
        "\n",
        "# Define schema of df_bronze\n",
        "message_schema = StructType([StructField('timestamp', TimestampType(), True),\n",
        "                             StructField('value', LongType(), True),\n",
        "                             StructField('event_type', StringType(), True),\n",
        "                             StructField('message_id', StringType(), True),\n",
        "                             StructField('channel', StringType(), True),\n",
        "                             StructField('country_id', IntegerType(), True),\n",
        "                             StructField('user_id', IntegerType(), True)])\n",
        "\n",
        "# readStream from bronze layer\n",
        "df_bronze_stream = spark.readStream.schema(message_schema).parquet(\"/content/lake/bronze/messages/data\")\n",
        "\n",
        "# Join of df_bronze with countries dataset\n",
        "df_bronze_with_country = df_bronze_stream.join(countries, on='country_id', how='left')"
      ],
      "metadata": {
        "id": "Uzsw4yKhi3bi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swvPj9hVpzNf"
      },
      "source": [
        "## Streaming Messages x Messages Corrupted"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing two streaming jobs, one for messages and another for messages_corrupted."
      ],
      "metadata": {
        "id": "0jJwBJi3eo95"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZAHIZeZMlpoH"
      },
      "outputs": [],
      "source": [
        "# Filter corrupted data (corrupted messages)\n",
        "df_corrupted = df_bronze_with_country.filter(\n",
        "    (F.col('event_type').isNull()) |\n",
        "    (F.col('event_type') == '') |\n",
        "    (F.col('event_type') == 'NONE')\n",
        ")\n",
        "\n",
        "# Filter non corrupted data (valid messages)\n",
        "df_valid = df_bronze_with_country.filter(\n",
        "    ~((F.col('event_type').isNull()) |\n",
        "      (F.col('event_type') == '') |\n",
        "      (F.col('event_type') == 'NONE'))\n",
        ")\n",
        "\n",
        "# Add column 'date' and write with partition by \"date\"\n",
        "def insert_corrupted_messages(df, batch_id):\n",
        "    df_with_date = df.withColumn(\"date\", F.to_date(F.col(\"timestamp\"))) # Add column 'date''\n",
        "    df_with_date.write.mode(\"append\").partitionBy(\"date\").format(\"parquet\").save(\"/content/lake/silver/messages_corrupted/data\") # partitionBy 'date'\n",
        "\n",
        "def insert_valid_messages(df, batch_id):\n",
        "    df_with_date = df.withColumn(\"date\", F.to_date(F.col(\"timestamp\")))\n",
        "    df_with_date.write.mode(\"append\").partitionBy(\"date\").format(\"parquet\").save(\"/content/lake/silver/messages/data\")\n",
        "\n",
        "\n",
        "# writeStream fo corrupted messages\n",
        "query_corrupted = (df_corrupted.writeStream\n",
        "    .outputMode('append')\n",
        "    .trigger(processingTime='5 seconds')  # trigger interval to 5 seconds\n",
        "    .foreachBatch(insert_corrupted_messages)\n",
        "    .option(\"checkpointLocation\", \"/content/lake/silver/messages_corrupted/checkpoint\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "# writeStream fo valid messages\n",
        "query_valid = (df_valid.writeStream\n",
        "    .outputMode('append')\n",
        "    .trigger(processingTime='5 seconds')  # trigger interval to 5 seconds\n",
        "    .foreachBatch(insert_valid_messages)\n",
        "    .option(\"checkpointLocation\", \"/content/lake/silver/messages/checkpoint\")\n",
        "    .start()\n",
        ")\n",
        "\n",
        "# Run the two streams\n",
        "query_corrupted.awaitTermination(20)  # run streaming for at least 20 seconds\n",
        "query_valid.awaitTermination(20)  # run streaming for at least 20 seconds\n",
        "\n",
        "# Stop the two streams\n",
        "query_corrupted.stop()\n",
        "query_valid.stop()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLK9jpjCu3xE"
      },
      "source": [
        "### Checking data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results from messages in bronze layer should match with the sum of messages+messages_corrupted in the silver layer."
      ],
      "metadata": {
        "id": "FUSbOTaEdKDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count records in Bronze Layer\n",
        "bronze_count = df_bronze.count()\n",
        "print(f\"Number of records in the bronze layer: {bronze_count}\")\n",
        "\n",
        "# Count records of valid messages in Silver Layer\n",
        "df_valid_messages = spark.read.format(\"parquet\").load(\"/content/lake/silver/messages/data\")\n",
        "valid_count = df_valid_messages.count()\n",
        "print(f\"Number of records in silver/messages (valid): {valid_count}\")\n",
        "\n",
        "# Count records of corrupted messages in Silver Layer\n",
        "df_corrupted_messages = spark.read.format(\"parquet\").load(\"/content/lake/silver/messages_corrupted/data\")\n",
        "corrupted_count = df_corrupted_messages.count()\n",
        "print(f\"Number of records in silver/messages_corrupted (corrupted): {corrupted_count}\")\n",
        "\n",
        "# Check if total records in bronze layer = valid messages in silver + corrupted messages in silver\n",
        "if bronze_count == (valid_count + corrupted_count):\n",
        "    print(\"The sum of messages in silver layer matches the number of records in the bronze layer.\")\n",
        "else:\n",
        "    print(\"The sum of messages in silver layer does not match the number of records in the bronze layer.\")\n"
      ],
      "metadata": {
        "id": "9tBQTd8TeUKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nk8seEvbmvcU"
      },
      "outputs": [],
      "source": [
        "df_valid_messages.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_corrupted_messages.show()"
      ],
      "metadata": {
        "id": "ixz91VDEdE-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfxIlBISSvRP"
      },
      "source": [
        "# Challenge 2\n",
        "\n",
        "- Run business report\n",
        "- But first, there is a bug in the system which is causing some duplicated messages, we need to exclude these lines from the report\n",
        "\n",
        "- Removing duplicates logic:\n",
        "  - Identify possible duplicates on message_id, event_type and channel\n",
        "  - In case of duplicates, consider only the first message (occurrence by timestamp)\n",
        "  - Ex:\n",
        "    In table below, the correct message to consider is the second line\n",
        "\n",
        "```\n",
        "    message_id | channel | event_type | timestamp\n",
        "    123        | CHAT    | CREATED    | 10:10:01\n",
        "    123        | CHAT    | CREATED    | 07:56:45 (first occurrence)\n",
        "    123        | CHAT    | CREATED    | 08:13:33\n",
        "```\n",
        "\n",
        "- After cleaning the data we're able to create the busines report"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "df_valid_messages = spark.read.format(\"parquet\").load(\"/content/lake/silver/messages/data\")"
      ],
      "metadata": {
        "id": "cdI4mU_Bt6Qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by message_id, event_type, and channel to check duplicates\n",
        "duplicates = df_valid_messages.groupBy(\"message_id\", \"event_type\", \"channel\").count().filter(\"count > 1\")\n",
        "\n",
        "duplicates.show(10,False)"
      ],
      "metadata": {
        "id": "VzxaWxHCt7Ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Show records with message_id 'af7712d1-6823-4c3a-8ff5-6b7f064b59bb' (duplicated record)\n",
        "df_valid_messages.filter(df_valid_messages.message_id == \"af7712d1-6823-4c3a-8ff5-6b7f064b59bb\").show(10, False)"
      ],
      "metadata": {
        "id": "8kwhNRC6umh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count total records before dedup\n",
        "total_before_dedup = df_valid_messages.count()\n",
        "print(f\"Total before deduplication: {total_before_dedup}\")"
      ],
      "metadata": {
        "id": "QjSIuhCkvIx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R3J9XyOHhqvU"
      },
      "outputs": [],
      "source": [
        "# Remove duplicated records\n",
        "dedup = df_valid_messages.withColumn(\"row_number\",\n",
        "                                     F.row_number()\\\n",
        "                                     .over(Window.partitionBy(\"message_id\", \"event_type\", \"channel\")\\\n",
        "                                     .orderBy(\"timestamp\"))).filter(\"row_number = 1\").drop(\"row_number\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Count total records after dedup\n",
        "total_after_dedup = dedup.count()\n",
        "print(f\"Total after deduplication: {total_after_dedup}\")"
      ],
      "metadata": {
        "id": "Mw2rULAIth0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by message_id, event_type, and channel to check duplicates\n",
        "duplicates_check = dedup.groupBy(\"message_id\", \"event_type\", \"channel\").count().filter(\"count > 1\")\n",
        "\n",
        "duplicates_check.show()\n"
      ],
      "metadata": {
        "id": "ZZXbEtQ9srlL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF9L9i25lk74"
      },
      "source": [
        "### Report 1\n",
        "  - Aggregate data by date, event_type and channel\n",
        "  - Count number of messages\n",
        "  - Pivot event_type from rows into columns\n",
        "  - Schema expected:\n",
        "  \n",
        "```\n",
        "|      date|channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
        "+----------+-------+-------+-------+----+--------+----+\n",
        "|2024-12-03|    SMS|      4|      4|   1|       1|   5|\n",
        "|2024-12-03|   CHAT|      3|      7|   5|       8|   4|\n",
        "|2024-12-03|   PUSH|   NULL|      3|   4|       3|   4|\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dedup.show(10,False)"
      ],
      "metadata": {
        "id": "ap2QiUa0w4qb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPHSMSXnTKgu"
      },
      "outputs": [],
      "source": [
        "# Aggregate data and count messages by date, event_type and channel\n",
        "qty_messages = dedup.groupBy(\"date\", \"channel\", \"event_type\").count()\n",
        "\n",
        "# Pivot event_type from rows into columns\n",
        "pivoted_event_type = qty_messages.groupBy(\"date\", \"channel\") \\\n",
        "    .pivot(\"event_type\") \\\n",
        "    .agg(F.sum(\"count\"))\n",
        "\n",
        "pivoted_event_type.show(10, False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxwOawo2lwQH"
      },
      "source": [
        "## Report 2\n",
        "\n",
        "- Identify the most active users by channel (sorted by number of iterations)\n",
        "- Schema expected:\n",
        "\n",
        "```\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|user_id|iterations|CHAT|EMAIL|OTHER|PUSH|SMS|\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|   1022|         5|   2|    0|    1|   0|  2|\n",
        "|   1004|         4|   1|    1|    1|   1|  0|\n",
        "|   1013|         4|   0|    0|    2|   1|  1|\n",
        "|   1020|         4|   2|    0|    1|   1|  0|\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsS7bkAJmWsW"
      },
      "outputs": [],
      "source": [
        "# Aggregate data and count by user_id and channel\n",
        "user_channel = dedup.groupBy(\"user_id\", \"channel\").count()\n",
        "\n",
        "# Pivot channel from rows into columns\n",
        "pivoted_channel = user_channel.groupBy(\"user_id\").pivot(\"channel\").sum(\"count\")\n",
        "\n",
        "# NULL substituted by 0\n",
        "pivoted_channel = pivoted_channel.fillna(0)\n",
        "\n",
        "# Total of iterations per user_id (iterations will be the sum of messages by channel per user_id)\n",
        "pivoted_channel = pivoted_channel.withColumn(\"iterations\", sum(F.col(c) for c in pivoted_channel.columns if c != \"user_id\"))\n",
        "\n",
        "# Reorder columns so iterations appears after user_id\n",
        "columns = [\"user_id\", \"iterations\"] + [col for col in pivoted_channel.columns if col not in [\"user_id\", \"iterations\"]]\n",
        "pivoted_channel = pivoted_channel.select(*columns)\n",
        "\n",
        "# Order (desc) user_id by iterations to see the most active user_id\n",
        "most_active_users = pivoted_channel.orderBy(F.desc(\"iterations\"))\n",
        "\n",
        "most_active_users.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9_kzDbDwDOS"
      },
      "source": [
        "# Challenge 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef0RjFTxwE5y"
      },
      "outputs": [],
      "source": [
        "# Theoretical question:\n",
        "\n",
        "# A new usecase requires the message data to be aggregate in near real time\n",
        "# They want to build a dashboard embedded in the platform website to analyze message data in low latency (few minutes)\n",
        "# This application will access directly the data aggregated by streaming process\n",
        "\n",
        "# Q1:\n",
        "- What would be your suggestion to achieve that using Spark Structure Streaming?\n",
        "Or would you choose a different data processing tool?\n",
        "\n",
        "- Which storage would you use and why? (database?, data lake?, kafka?)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the new usecase requires near real time, we could opt for a tool that provides micro-batch processing. As we need to aggregate data (a stateful operation), we can use **Spark Structure Streaming**, even if its low latency (few seconds to a few minutes of delay) is not as low as with a real-time streaming tool. For the refered case, Spark Structure Streaming seems to serve the required purpose.\n",
        "\n",
        "For storage, I would say the best option is to use a **datalake**. It would be able to handle the aggregated data that is going to be visualized in the dashboard, something that the database would not be able to do as good. Regarding Kafka, it is said it is not designed for long-term data storage or complex querying, besides being expensive."
      ],
      "metadata": {
        "id": "OVJfODxybwjK"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "d9LeYFsPTjAb",
        "cDGMKwBdi1qy"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}